version: '3.5'


# The services are listed in this order
# DataBases
# Kafka
# Spark
# Python

services:

  # DATABASES

  stock-input-mongo:
    image: mongo
    container_name: stock-input-mongo
    expose:
      - 27017
    ports:
      - "127.0.0.1:27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=twitter_sources
    env_file:
      - ./env_files/.env-mongo-input
    volumes:
      - mongo_input_volume:/data/db
    networks:
      - input_network
    restart: unless-stopped

  stock-output-mongo:
    image: mongo
    container_name: stock-output-mongo
    expose:
      - 27017
    ports:
      - "127.0.0.1:27018:27017"
    environment:
      - MONGO_INITDB_DATABASE=text_databases
    env_file:
      - ./env_files/.env-mongo-output
    volumes:
      - mongo_output_volume:/data/db
    networks:
      - input_network
      - output_network
    restart: unless-stopped

  stock-input-postgres:
    image: debezium/postgres:11
    container_name: stock-input-postgres
    expose:
      - 5432
    ports:
      - "127.0.0.1:5432:5432"
    environment:
      - PGDATA=/data/postgres
      - POSTGRES_DB=stocks
    env_file:
      - ./env_files/.env-postgres-input
    volumes:
      - postgres_stock_input:/data/postgres
      - ./misc/create_schema_postgres_input.sql:/docker-entrypoint-initdb.d/create_schema_postgres_input.sql
    networks:
      - input_network
    restart: unless-stopped

  stock-output-postgres:
    image: debezium/postgres:11
    container_name: stock-output-postgres
    expose:
      - 5432
    ports:
      - "127.0.0.1:5433:5432"
    environment:
      - POSTGRES_DB=stocks
      - PGDATA=/data/postgres
    env_file:
      - ./env_files/.env-postgres-output
    volumes:
      - postgres_stock_output:/data/postgres
      - ./misc/create_schema_postgres_output.sql:/docker-entrypoint-initdb.d/create_schema_postgres_output.sql
    networks:
      - input_network
      - output_network
    restart: unless-stopped

  # KAFKA SERVICES

  stock-zookeeper:
    image: confluentinc/cp-zookeeper:5.1.2
    user: root
    container_name: stock-zookeeper
    expose:
      - 2181
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    networks:
      - input_network
      - output_network

  stock-kafka:
    image: confluentinc/cp-kafka:5.1.2
    user: root
    container_name: stock-kafka
    expose:
      - 9092
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_LISTENERS=PLAINTEXT://:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://stock-kafka:9092
      - KAFKA_ZOOKEEPER_CONNECT=stock-zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    depends_on:
      - stock-input-mongo
      - stock-input-postgres
      - stock-output-mongo
      - stock-output-postgres
      - stock-zookeeper
    networks:
      - input_network
      - output_network

  stock-schema-registry:
    image: confluentinc/cp-schema-registry:5.1.2
    container_name: stock-schema-registry
    hostname: schema-registry
    expose:
      - 8081
    depends_on:
      - stock-zookeeper
      - stock-kafka
    environment:
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=PLAINTEXT://stock-kafka:9092
      - SCHEMA_REGISTRY_HOST_NAME=stock-schema-registry
      - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=stock-zookeeper:2181
      - SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081
    networks:
      - input_network
      - output_network

  stock-ksql-server:
    image: confluentinc/cp-ksql-server:5.1.2
    container_name: stock-ksql-server
    depends_on:
      - stock-kafka
      - stock-schema-registry
    environment:
      - KSQL_BOOTSTRAP_SERVERS=stock-kafka:9092
      - KSQL_LISTENERS=http://0.0.0.0:8088
      - KSQL_KSQL_SCHEMA_REGISTRY_URL=http://stock-schema-registry:8081
      - KSQL_KSQL_SERVICE_ID=stock-ksql-server
    networks:
      - input_network
      - output_network

  stock-connect:
    image: debezium-connect  # built from debezium/connect:0.10
    build:
      context: ./kafka
      dockerfile: ./connect/debezium.Dockerfile
    container_name: stock-connect
    depends_on:
      - stock-zookeeper
      - stock-kafka
      - stock-input-postgres
      - stock-output-postgres
    expose:
      - 8083
    environment:
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my-connect-configs
      - OFFSET_STORAGE_TOPIC=my-connect-offsets
      - ADVERTISED_HOST_NAME=stock-connect
      - BOOTSTRAP_SERVERS=stock-kafka:9092
      - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverte
    networks:
      - input_network
      - output_network

  # SPARK SERVICES

  stock-spark-master:
    image: docker.io/bitnami/spark:3-debian-10
    container_name: stock-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - output_network
    expose:
      - 8080
      - 7077

  stock-spark-worker:
    image: docker.io/bitnami/spark:3-debian-10
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://stock-spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - stock-spark-master
    networks:
      - output_network

  spark-client:
    image: docker.io/bitnami/spark:3-debian-10
    entrypoint:
      - sleep
      - infinity
    depends_on:
      - stock-spark-master
      - stock-spark-worker
    networks:
      - output_network

  # PYTHON SERVICES

  stock-python-analysis:
    image: stock-python-analysis
    build:
      context: ./python
      dockerfile: ./dockerfiles/analyzer.Dockerfile
    container_name: stock-python-analysis
    networks:
      - output_network

  stock-input-generator:
    image: stock-input-generator
    build:
      context: ./python
      dockerfile: ./dockerfiles/generator.Dockerfile
    container_name: stock-input-generator
    environment:
      - POSTGRES_INPUT_DB=stocks
      - POSTGRES_INPUT_PORT=5432
      - POSTGRES_INPUT_HOST=stock-input-postgres
    networks:
      - input_network
    #command: ./src/generator/__main__.py


networks:
  input_network:
    name: "Input Network"
  output_network:
    name: "Output Network"

volumes:
  mongo_input_volume:
  mongo_output_volume:
  postgres_stock_input:
  postgres_stock_output:
